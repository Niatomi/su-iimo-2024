{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2588f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "def assess_tendency_by_metric(dataset, metric='silhouette', n_cluster=10, random_state=None):\n",
    "    \"\"\"Assess the clusterability of a dataset using KMeans algorithm and a metric score, the best cluster number\n",
    "       is the number that best scored with the silhouette score.\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       dataset : numpy array, DataFrame\n",
    "           The input dataset\n",
    "       metric : string\n",
    "            The method to assess cluster quality ('silhouette', 'calinski_harabasz', 'davies_bouldin'), default to\n",
    "            'silhouette'\n",
    "       n_cluster : int\n",
    "           The maxium number of cluster to consider\n",
    "       random_state : int (default to None)\n",
    "\n",
    "       Returns\n",
    "       ---------------------\n",
    "       (n_clusters, value) :  n_clusters is the number of cluster that best scored on the silhouette score on Kmeans.\n",
    "       As for value, it is the silhouette score for each number of cluster on KMeans.\n",
    "\n",
    "       Examples\n",
    "       --------\n",
    "       >>> from sklearn import datasets\n",
    "       >>> from pyclustertend import assess_tendency_by_metric\n",
    "       >>> from sklearn.preprocessing import scale\n",
    "       >>> X = scale(datasets.load_boston().data)\n",
    "       >>> assess_tendency_by_metric(X, n_cluster=10)\n",
    "       (2, array([0.36011769, 0.25740335, 0.28098046, 0.28781574, 0.26746932,\n",
    "           0.26975514, 0.27155699, 0.28883395, 0.29028124]))\n",
    "\n",
    "       \"\"\"\n",
    "    result_kmeans = np.array([])\n",
    "\n",
    "    for k_cluster in range(2, n_cluster + 1):\n",
    "        labels = KMeans(n_clusters=k_cluster,\n",
    "                        random_state=random_state).fit_predict(dataset)\n",
    "        if metric == 'silhouette':\n",
    "            result_kmeans = np.append(result_kmeans, silhouette_score(dataset, labels))\n",
    "        elif metric == 'calinski_harabasz':\n",
    "            result_kmeans = np.append(result_kmeans, calinski_harabasz_score(dataset, labels))\n",
    "        elif metric == 'davies_bouldin':\n",
    "            result_kmeans = np.append(result_kmeans, davies_bouldin_score(dataset, labels))\n",
    "\n",
    "    if metric == 'davies_bouldin':\n",
    "        return np.argmin(result_kmeans) + 2, result_kmeans\n",
    "    else:\n",
    "        return np.argmax(result_kmeans) + 2, result_kmeans\n",
    "\n",
    "\n",
    "\n",
    "def assess_tendency_by_mean_metric_score(dataset, n_cluster=10, random_state=None):\n",
    "    \"\"\"Assess the clusterability of a dataset using KMeans algorithm and the silhouette, calinski and davies bouldin\n",
    "    score, the best cluster number is the mean of the result of the three methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : numpy array, DataFrame\n",
    "        The input dataset\n",
    "    n_cluster : int\n",
    "        The maxium number of cluster to consider\n",
    "    random_state : int (default to None)\n",
    "\n",
    "    Returns\n",
    "    ---------------------\n",
    "    n_clusters :  n_clusters is the mean of the best number of cluster score (with Kmeans algorithm)\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn import datasets\n",
    "    >>> from pyclustertend import assess_tendency_by_mean_metric_score\n",
    "    >>> from sklearn.preprocessing import scale\n",
    "    >>> X = scale(datasets.load_boston().data)\n",
    "    >>> assess_tendency_by_mean_metric_score(X,10)\n",
    "    2.6666666666666665\n",
    "    \"\"\"\n",
    "\n",
    "    silhouette_best, _ = assess_tendency_by_metric(dataset,\n",
    "                                                   metric='silhouette',\n",
    "                                                   n_cluster=n_cluster,\n",
    "                                                   random_state=random_state)\n",
    "\n",
    "    calinski_best, _ = assess_tendency_by_metric(dataset,\n",
    "                                                 metric='calinski_harabasz',\n",
    "                                                 n_cluster=n_cluster,\n",
    "                                                 random_state=random_state)\n",
    "\n",
    "    davies_best, _ = assess_tendency_by_metric(dataset,\n",
    "                                               metric='davies_bouldin',\n",
    "                                               n_cluster=n_cluster,\n",
    "                                               random_state=random_state)\n",
    "\n",
    "    return np.mean([silhouette_best, calinski_best, davies_best])\n",
    "\n",
    "\n",
    "def hopkins(data_frame, sampling_size):\n",
    "    \"\"\"Assess the clusterability of a dataset. A score between 0 and 1, a score around 0.5 express\n",
    "    no clusterability and a score tending to 0 express a high cluster tendency.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_frame : numpy array\n",
    "        The input dataset\n",
    "    sampling_size : int\n",
    "        The sampling size which is used to evaluate the number of DataFrame.\n",
    "\n",
    "    Returns\n",
    "    ---------------------\n",
    "    score : float\n",
    "        The hopkins score of the dataset (between 0 and 1)\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn import datasets\n",
    "    >>> from pyclustertend import hopkins\n",
    "    >>> X = datasets.load_iris().data\n",
    "    >>> hopkins(X,150)\n",
    "    0.16\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data_frame) == np.ndarray:\n",
    "        data_frame = pd.DataFrame(data_frame)\n",
    "\n",
    "    # Sample n observations from D : P\n",
    "\n",
    "    if sampling_size > data_frame.shape[0]:\n",
    "        raise Exception(\n",
    "            'The number of sample of sample is bigger than the shape of D')\n",
    "\n",
    "    data_frame_sample = data_frame.sample(n=sampling_size)\n",
    "\n",
    "    # Get the distance to their neirest neighbors in D : X\n",
    "\n",
    "    tree = BallTree(data_frame, leaf_size=2)\n",
    "    dist, _ = tree.query(data_frame_sample, k=2)\n",
    "    data_frame_sample_distances_to_nearest_neighbours = dist[:, 1]\n",
    "\n",
    "    # Randomly simulate n points with the same variation as in D : Q.\n",
    "\n",
    "    max_data_frame = data_frame.max()\n",
    "    min_data_frame = data_frame.min()\n",
    "\n",
    "    uniformly_selected_values_0 = np.random.uniform(min_data_frame[0], max_data_frame[0], sampling_size)\n",
    "    uniformly_selected_values_1 = np.random.uniform(min_data_frame[1], max_data_frame[1], sampling_size)\n",
    "\n",
    "    uniformly_selected_observations = np.column_stack((uniformly_selected_values_0, uniformly_selected_values_1))\n",
    "    if len(max_data_frame) >= 2:\n",
    "        for i in range(2, len(max_data_frame)):\n",
    "            uniformly_selected_values_i = np.random.uniform(min_data_frame[i], max_data_frame[i], sampling_size)\n",
    "            to_stack = (uniformly_selected_observations, uniformly_selected_values_i)\n",
    "            uniformly_selected_observations = np.column_stack(to_stack)\n",
    "\n",
    "    uniformly_selected_observations_df = pd.DataFrame(uniformly_selected_observations)\n",
    "\n",
    "    # Get the distance to their neirest neighbors in D : Y\n",
    "\n",
    "    tree = BallTree(data_frame, leaf_size=2)\n",
    "    dist, _ = tree.query(uniformly_selected_observations_df, k=1)\n",
    "    uniformly_df_distances_to_nearest_neighbours = dist\n",
    "\n",
    "    # return the hopkins score\n",
    "\n",
    "    x = sum(data_frame_sample_distances_to_nearest_neighbours)\n",
    "    y = sum(uniformly_df_distances_to_nearest_neighbours)\n",
    "\n",
    "    if x + y == 0:\n",
    "        raise Exception('The denominator of the hopkins statistics is null')\n",
    "\n",
    "    return x / (x + y)[0]\n",
    "\n",
    "def vat(data, return_odm=False, figure_size=(10, 10)):\n",
    "    \"\"\"VAT means Visual assesement of tendency. basically, it allow to asses cluster tendency\n",
    "    through a map based on the dissimiliraty matrix.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    data : matrix\n",
    "        numpy array\n",
    "\n",
    "    return_odm : return the Ordered Dissimalirity Matrix\n",
    "        boolean (default to False)\n",
    "\n",
    "    figure_size : size of the VAT.\n",
    "        tuple (default to (10,10))\n",
    "\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "\n",
    "    ODM : matrix\n",
    "        the ordered dissimalarity matrix plotted.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ordered_dissimilarity_matrix = compute_ordered_dissimilarity_matrix(data)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=figure_size)\n",
    "    ax.imshow(ordered_dissimilarity_matrix, cmap='gray', vmin=0, vmax=np.max(ordered_dissimilarity_matrix))\n",
    "\n",
    "    if return_odm is True:\n",
    "        return ordered_dissimilarity_matrix\n",
    "\n",
    "\n",
    "\n",
    "def compute_ordered_dissimilarity_matrix(X):\n",
    "    \"\"\"The ordered dissimilarity matrix is used by visual assesement of tendency. It is a just a a reordering\n",
    "    of the dissimilarity matrix.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : matrix\n",
    "        numpy array\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "\n",
    "    ODM : matrix\n",
    "        the ordered dissimalarity matrix .\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1 :\n",
    "\n",
    "    observation_path = []\n",
    "\n",
    "    matrix_of_pairwise_distance = pairwise_distances(X)\n",
    "    list_of_int = np.zeros(matrix_of_pairwise_distance.shape[0], dtype=\"int\")\n",
    "\n",
    "    index_of_maximum_value = np.argmax(matrix_of_pairwise_distance)\n",
    "\n",
    "    column_index_of_maximum_value = index_of_maximum_value // matrix_of_pairwise_distance.shape[1]\n",
    "\n",
    "    list_of_int[0] = column_index_of_maximum_value\n",
    "    observation_path.append(column_index_of_maximum_value)\n",
    "\n",
    "    K = np.linspace(0, matrix_of_pairwise_distance.shape[0] - 1, matrix_of_pairwise_distance.shape[0], dtype=\"int\")\n",
    "    J = np.delete(K, column_index_of_maximum_value)\n",
    "\n",
    "    # Step 2 :\n",
    "\n",
    "    for r in range(1, matrix_of_pairwise_distance.shape[0]):\n",
    "\n",
    "        p, q = (-1, -1)\n",
    "\n",
    "        mini = np.max(matrix_of_pairwise_distance)\n",
    "\n",
    "        for candidate_p in observation_path:\n",
    "            for candidate_j in J:\n",
    "                if matrix_of_pairwise_distance[candidate_p, candidate_j] < mini:\n",
    "                    p = candidate_p\n",
    "                    q = candidate_j\n",
    "                    mini = matrix_of_pairwise_distance[p, q]\n",
    "\n",
    "        list_of_int[r] = q\n",
    "        observation_path.append(q)\n",
    "\n",
    "        ind_q = np.where(np.array(J) == q)[0][0]\n",
    "        J = np.delete(J, ind_q)\n",
    "\n",
    "    # Step 3\n",
    "\n",
    "    ordered_matrix = np.zeros(matrix_of_pairwise_distance.shape)\n",
    "\n",
    "    for column_index_of_maximum_value in range(ordered_matrix.shape[0]):\n",
    "        for j in range(ordered_matrix.shape[1]):\n",
    "            ordered_matrix[column_index_of_maximum_value, j] = matrix_of_pairwise_distance[\n",
    "                list_of_int[column_index_of_maximum_value], list_of_int[j]]\n",
    "\n",
    "    # Step 4 :\n",
    "\n",
    "    return ordered_matrix\n",
    "\n",
    "\n",
    "\n",
    "def ivat(data, return_odm=False, figure_size=(10, 10)):\n",
    "    \"\"\"iVat return a visualisation based on the Vat but more reliable and easier to\n",
    "    interpret.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    data : matrix\n",
    "        numpy array\n",
    "\n",
    "    return_odm : return the Ordered Dissimalirity Matrix\n",
    "            boolean (default to False)\n",
    "\n",
    "    figure_size : size of the VAT.\n",
    "        tuple (default to (10,10))\n",
    "\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "\n",
    "    D_prim : matrix\n",
    "        the ivat ordered dissimalarity matrix.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ordered_matrix = compute_ivat_ordered_dissimilarity_matrix(data)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=figure_size)\n",
    "    ax.imshow(ordered_matrix, cmap='gray', vmin=0, vmax=np.max(ordered_matrix))\n",
    "\n",
    "    if return_odm is True:\n",
    "        return ordered_matrix\n",
    "\n",
    "\n",
    "\n",
    "def compute_ivat_ordered_dissimilarity_matrix(X):\n",
    "    \"\"\"The ordered dissimilarity matrix is used by ivat. It is a just a a reordering\n",
    "    of the dissimilarity matrix.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : matrix\n",
    "        numpy array\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "\n",
    "    D_prim : matrix\n",
    "        the ordered dissimalarity matrix .\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ordered_matrix = compute_ordered_dissimilarity_matrix(X)\n",
    "    re_ordered_matrix = np.zeros((ordered_matrix.shape[0], ordered_matrix.shape[0]))\n",
    "\n",
    "    for r in range(1, ordered_matrix.shape[0]):\n",
    "        # Step 1 : find j for which D[r,j] is minimum and j in [1:r-1]\n",
    "\n",
    "        j = np.argmin(ordered_matrix[r, 0:r])\n",
    "\n",
    "        # Step 2 :\n",
    "\n",
    "        re_ordered_matrix[r, j] = ordered_matrix[r, j]\n",
    "\n",
    "        # Step 3 : pour c : 1,r-1 avec c !=j\n",
    "        c_tab = np.array(range(0, r))\n",
    "        c_tab = c_tab[c_tab != j]\n",
    "\n",
    "        for c in c_tab:\n",
    "            re_ordered_matrix[r, c] = max(ordered_matrix[r, j], re_ordered_matrix[j, c])\n",
    "            re_ordered_matrix[c, r] = re_ordered_matrix[r, c]\n",
    "\n",
    "    return re_ordered_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c57471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
